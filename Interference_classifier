import pickle  
import cv2  
import mediapipe as mp  
import numpy as np  
import time  
import pyttsx3  # Biblioteca para conversão de texto em fala  
import threading  # Biblioteca para executar a fala em uma thread separada  
  
# Inicializar o mecanismo de voz  
engine = pyttsx3.init()  
engine.setProperty('rate', 150)  # Velocidade da fala  
  
# Função para falar em uma thread separada  
def speak(text):  
   engine.say(text)  
   engine.runAndWait()  
  
# Carregar o modelo treinado  
model_dict = pickle.load(open('./model.p', 'rb'))  
model = model_dict['model']  
  
cap = cv2.VideoCapture(0)  
  
mp_hands = mp.solutions.hands  
mp_drawing = mp.solutions.drawing_utils  
mp_drawing_styles = mp.solutions.drawing_styles  
  
hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)  
  
# Dicionário de labels (ajuste conforme necessário)  
labels_dict = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X'}  
  
# Variáveis para controle de tempo  
prediction_time = 1  # Intervalo de 1 segundo entre previsões  
last_audio_time = time.time()  # Armazena o último tempo em que o áudio foi emitido  
audio_interval = 2  # Intervalo de 2 segundos para falar a predição  
hand_visible_time = 1  # Tempo que a mão deve estar visível antes da previsão  
hand_detected_time = None  # Tempo em que a mão foi detectada  
  
while True:  
   data_aux = []  
   x_ = []  
   y_ = []  
  
   ret, frame = cap.read()  
  
   H, W, _ = frame.shape  
  
   frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  
  
   results = hands.process(frame_rgb)  
   if results.multi_hand_landmarks:  
      # Mão detectada  
      if hand_detected_time is None:  
        hand_detected_time = time.time()  # Armazena o tempo em que a mão foi detectada  
      else:  
        # Verifica se a mão está visível por mais de hand_visible_time  
        if time.time() - hand_detected_time >= hand_visible_time:  
           for hand_landmarks in results.multi_hand_landmarks:  
              mp_drawing.draw_landmarks(  
                frame,  # imagem para desenhar  
                hand_landmarks,  # saída do modelo  
                mp_hands.HAND_CONNECTIONS,  # conexões da mão  
                mp_drawing_styles.get_default_hand_landmarks_style(),  
                mp_drawing_styles.get_default_hand_connections_style())  
  
           for hand_landmarks in results.multi_hand_landmarks:  
              for i in range(len(hand_landmarks.landmark)):  
                x = hand_landmarks.landmark[i].x  
                y = hand_landmarks.landmark[i].y  
  
                x_.append(x)  
                y_.append(y)  
  
              # Adicionar as coordenadas X e Y normalizadas  
              for i in range(len(hand_landmarks.landmark)):  
                x = hand_landmarks.landmark[i].x  
                y = hand_landmarks.landmark[i].y  
                data_aux.append(x - min(x_))  
                data_aux.append(y - min(y_))  
  
           # Certifique-se de que temos exatamente 42 features (21 landmarks * 2)  
           data_aux = data_aux[:42]  # Trunca ou ajusta para ter exatamente 42 features  
  
           # Realizar a predição usando o modelo  
           prediction = model.predict([np.asarray(data_aux)])  
           predicted_character = labels_dict[int(prediction[0])]  
  
           # Verificar se o intervalo de tempo para o áudio foi atingido  
           current_time = time.time()  
           if current_time - last_audio_time >= audio_interval:  
              # Atualizar o tempo do último áudio  
              last_audio_time = current_time  
  
              # Iniciar a fala em uma nova thread para evitar bloqueios  
              threading.Thread(target=speak, args=(predicted_character,)).start()  
  
           # Obtenha as coordenadas para desenhar a caixa de texto  
           x1 = int(min(x_) * W) - 10  
           y1 = int(min(y_) * H) - 10  
           x2 = int(max(x_) * W) - 10  
           y2 = int(max(y_) * H) - 10  
  
           # Desenhar a caixa ao redor da mão e a predição do caractere  
           cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)  
           cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (255, 0, 0), 3, cv2.LINE_AA)  
  
           # Calcular as coordenadas X, Y, Z do ponto médio da mão  
           x_mid = (x1 + x2) / 2  
           y_mid = (y1 + y2) / 2  
           z_mid = 0  # Como não temos informações de profundidade, definimos Z como 0  
  
           # Exibir as coordenadas no topo da tela  
           cv2.putText(frame, f"X: {x_mid}, Y: {y_mid}, Z: {z_mid}", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)  
  
   else:  
      # Se nenhuma mão foi detectada, resetar o tempo de detecção  
      hand_detected_time = None  # Redefine o tempo se a mão não estiver visível  
  
   cv2.imshow('frame', frame)  
   cv2.waitKey(1)  
  
cap.release()  
cv2.destroyAllWindows()
